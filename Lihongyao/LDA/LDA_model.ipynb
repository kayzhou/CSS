{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97e3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "from random import seed\n",
    "\n",
    "import gensim\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, ldamodel\n",
    "from pip import main\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=23)\n",
    "\n",
    "\n",
    "def read_dict_corpus():\n",
    "    print(\"读取文件中 >>> >>>\")\n",
    "    data = pd.read_csv(\"all_text.csv\")[\"seg\"].unique()\n",
    "    data = [seg.split() for seg in np.random.choice(data, 10000000)]\n",
    "    print(\"文本量：\", len(data))\n",
    "    # 取得词表\n",
    "    stw = set([line.strip() for line in open('stw_2000.txt', encoding='utf-8')])\n",
    "    print(\"读取完成！\")\n",
    "    \n",
    "    dictionary = Dictionary()\n",
    "\n",
    "    with open(\"data/train_data_2022.04.30.txt\", \"w\") as f:\n",
    "        for doc in tqdm(data):\n",
    "            words = [w for w in doc if w not in stw and len(w) > 1]\n",
    "            if len(words) > 0:\n",
    "                f.write(\" \".join(words) + \"\\n\")\n",
    "                dictionary.add_documents([words])\n",
    "                      \n",
    "    dictionary.filter_extremes(no_below=100, no_above=0.5, keep_n=50000)\n",
    "    dictionary.save_as_text(\"data/dict_2022.04.30.txt\")\n",
    "\n",
    "    # 取得语料库\n",
    "    corpus = [(i, dictionary.doc2bow(doc)) for i, doc in enumerate(data)]\n",
    "    corpus = [bow for _, bow in corpus if bow]\n",
    "    print(len(corpus), len(corpus))\n",
    "\n",
    "    del data\n",
    "    #建立TF-IDF模型\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    #使用TF-IDF值为词项进行加权\n",
    "    corpus = tfidf[corpus]\n",
    "    # corpus_tfidf\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "\n",
    "def my_lda(dictionary, corpus, num_t):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        dictionary (_type_): 词典\n",
    "        corpus (_type_): 语料库\n",
    "        num_t (_type_): 主题数\n",
    "    \"\"\"\n",
    "    print(\"lda training ... # of Topics:\", num_t)\n",
    "    lda_model = ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_t,\n",
    "        passes=2,\n",
    "        alpha='auto',\n",
    "        eta='auto')\n",
    "    # 保存训练好的lda\n",
    "    lda_model.save(f'model/lda_{num_t}.model')\n",
    "    print(\"保存模型！\")\n",
    "    # 调用保存好的lda模型\n",
    "    # lda_model = models.ldamodel.LdaModel.load(f'model/lda_{num_t}.model')\n",
    "\n",
    "    # 将主题、词和概率保存到csv文件\n",
    "    with open(f\"data/topic_{num_t}.csv\", \"w\", encoding=\"utf-8\", newline='') as csvfile:\n",
    "        fieldnames = [\"topic_id\", \"term\", \"prob\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for topic_id in range(num_t):\n",
    "            term_probs = lda_model.show_topic(topic_id, topn=50)\n",
    "            for term, prob in term_probs:\n",
    "                row = {}\n",
    "                row['topic_id'] = topic_id\n",
    "                row['prob'] = prob\n",
    "                row['term'] = term\n",
    "                writer.writerow(row)\n",
    "            \n",
    "            \n",
    "\n",
    "def apply_my_lda(data_pth, lda_model_pth):\n",
    "    lda = models.ldamodel.LdaModel.load('model/lda_6.model')\n",
    "    dictionary = Dictionary.load_from_text(\"data/dict_2022.04.30.txt\")\n",
    "    \n",
    "    df = pd.read_csv(data_pth)\n",
    "    seg = df['seg']\n",
    "    blog_time = df['blog_time'][0]\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(seg)):\n",
    "        test_doc = seg[i]\n",
    "        test_doc = [word for word in test_doc.split()]\n",
    "        # 文本转换成bow\n",
    "        doc_bow = dictionary.doc2bow(test_doc)\n",
    "        doc_lda = lda.get_document_topics(doc_bow)\n",
    "        doc_lda = dict(doc_lda)\n",
    "        # 找到最大概率对应的主题\n",
    "        doc_topic = max(doc_lda, key=doc_lda.get)\n",
    "        result.append(doc_topic)\n",
    "\n",
    "    # print(result)\n",
    "    result_dict = Counter(result)\n",
    "    result_dict = {\n",
    "        'date': blog_time,\n",
    "        'topic0': result_dict[0],\n",
    "        'topic1': result_dict[1],\n",
    "        'topic2': result_dict[2],\n",
    "        'topic3': result_dict[3],\n",
    "        'topic4': result_dict[4],\n",
    "        'topic5': result_dict[5]\n",
    "    }\n",
    "\n",
    "    target = pd.DataFrame(result_dict, index=[-1])\n",
    "    return target\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # dictionary, corpus = read_dict_corpus()\n",
    "    # my_lda(dictionary, corpus, 4)\n",
    "    # my_lda(dictionary, corpus, 6)\n",
    "\n",
    "    ## 调用保存好的lda模型\n",
    "    base_pth = 'data/break_text/'\n",
    "    csv_list = os.listdir(base_pth)\n",
    "    csv_list = [base_pth + i for i in csv_list]\n",
    "\n",
    "    target_csv = pd.DataFrame(columns=['date', 'topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "\n",
    "    for _csv in csv_list:\n",
    "        # try:\n",
    "        #    print(f\"正在执行:{_csv}\")\n",
    "        #    temp_csv = apply_my_lda(_csv, \"model/lda_6.model\")\n",
    "        #    target_csv = target_csv.append(temp_csv)\n",
    "        #except Exception as e:\n",
    "        #    print(f\"发生错误：{e}\")\n",
    "        print(f\"正在执行:{_csv}\")\n",
    "        temp_csv = apply_my_lda(_csv, \"model/lda_6.model\")\n",
    "        target_csv = target_csv.append(temp_csv)\n",
    "    \n",
    "    target_csv.to_csv('data/lda_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dda14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368aa88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4ed90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc09408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71e81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
